import numpy as np
import random 
import matplotlib.pyplot as plt

#parameters
N = 100          # number of agents
T = 200          # time steps
alpha = 0.5      # crowd penalty
beta = 0.1       # action penalty
gamma = 0.9      # discount factor
epsilon = 0.2    # exploration rate
lr = 0.1   

#@ each state agent can choose to go back -1, stay, or move foward 1
actions = np.array([-1,0,1])
states = np.arange(-11,10)
Q = np.zeros((len(states), len(actions)))

def choose_action(state_idx):
    if np.random.rand() < epsilon:
       return np.random.choice(len(actions))
    else:
        return np.argmax(Q[state_idx])
    
def get_state_index(x):
    return np.clip(int(x) + 10, 0, len(states)-1)

#initilize position
x = np.random.randint(-5, 6, N)
x_hist = [x.copy()]


for t in range(T):
    dist = np.mean(x)
    for i in range(N):
       current_state = round(x[i])
       a_idx = choose_action(current_state)
       a = actions[a_idx]

       #apply action to current state
       x_new = current_state + a
       #reward function rt ​= −(∣xti​∣+α∣xti​−xˉt​∣+β∣ati​∣)
       reward = - (abs(x_new) + alpha * abs(x_new - dist) + beta * abs(x_new))

       #next state
       x_next = get_state_index(x_new)

       Q[current_state, a_idx] = Q[current_state, a_idx] + lr * (
            reward + gamma * np.max(Q[x_next]) - Q[current_state, a_idx]
        )
       x[i] = x_new
       
    x_hist.append(x.copy())

print(Q)
# Plot results
x_hist = np.array(x_hist)
plt.figure(figsize=(8,5))
plt.plot(np.mean(x_hist, axis=1))
plt.title("Mean Position of Agents over Time")
plt.xlabel("Time")
plt.ylabel("Mean Position")
plt.show()

plt.hist(x_hist[-1], bins=15, alpha=0.7)
plt.title("Final Distribution of Agent Positions")
plt.xlabel("Position")
plt.ylabel("Count")
plt.show()