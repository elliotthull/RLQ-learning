{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = np.array([0,1])\n",
    "actions = np.array([0,1]) #0 = stay , #1 = move\n",
    "N = 200000 #number of steps\n",
    "phi = 500 #soft-min parameter\n",
    "p = 0.01\n",
    "c = 5\n",
    "gamma = 0.5\n",
    "tol_q = 0.1\n",
    "tol_mu = 0.01\n",
    "om_q = 0.85\n",
    "om_mu = 0.55\n",
    "\n",
    "Q = np.zeros((N+1, 2, 2))\n",
    "Q[0,:,:] = [[10,10],[10,10]]\n",
    "\n",
    "mu = np.zeros((N+1, 2, 2, 2))\n",
    "for x in range(2):\n",
    "    for a in range(2):\n",
    "        mu[0, x, a] = [0.5, 0.5]\n",
    "\n",
    "\n",
    "def rhosCalc (count_xa, n):\n",
    "    rhoQ = 1 / np.pow(1 + count_xa, om_q)\n",
    "    rhoMu = 1 / np.pow(2 + n, om_mu)\n",
    "    return { 'q': rhoQ, 'mu': rhoMu }\n",
    "\n",
    "def env(state, action, mu):\n",
    "    newS = state\n",
    "    if random.random() > p:\n",
    "        newS = (state + action) % 2\n",
    "    else: \n",
    "        newS = (state + (1-action)) % 2\n",
    "\n",
    "    return { \"newState\": newS,  \"cost\": newS + (c * mu[0])}\n",
    "\n",
    "def stable_softmin_action(q_values, actions, phi=phi):\n",
    "\n",
    "    z = -phi * np.array(q_values)\n",
    "    z -= np.max(z)\n",
    "    w = np.exp(z)\n",
    "    probs = w / np.sum(w)\n",
    "\n",
    "    if not np.all(np.isfinite(probs)):\n",
    "        probs = np.ones(len(actions)) / len(actions)\n",
    "\n",
    "    action = np.random.choice(actions, p=probs)\n",
    "\n",
    "    return action, probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize count for finding rho_Q (learning rate)\n",
    "count = [[0, 0],[0,0]]\n",
    "\n",
    "x = np.random.choice(states)\n",
    "mu[0] = [1,0] if x == 0 else [0,1]\n",
    "\n",
    "# iterate N times\n",
    "for n in range(N):\n",
    "\n",
    "    # copy new Q matrix, mu distribution\n",
    "    Q[n + 1] = Q[n].copy()\n",
    "    mu[n + 1] = mu[n].copy()\n",
    "    # use softmin to choose next action\n",
    "    a, probs = stable_softmin_action(Q[n][x], actions)\n",
    "    # update counter for rho calculation\n",
    "    \n",
    "    count[x][a] += 1\n",
    "    rhos = rhosCalc(count[x][a], n)\n",
    "\n",
    "    # input state, action, mu into environment\n",
    "    # and receive new state and cost\n",
    "    envir = env(x, a, mu[n][x][a])\n",
    "\n",
    "    New_x = envir[\"newState\"]\n",
    "    delta_X_n = np.zeros(len(states))\n",
    "    delta_X_n[New_x] = 1\n",
    "    mu[n + 1][x][a] = mu[n][x][a] + rhos['mu'] * (delta_X_n - mu[n][x][a])\n",
    "\n",
    "    # update Q matrix, current state, mu distribution\n",
    "    Q[n+1][x][a] = Q[n][x][a] + rhos['q'] * (envir[\"cost\"] + gamma * (np.min(Q[n][New_x])) - Q[n][x][a])\n",
    "\n",
    "    x = New_x\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
